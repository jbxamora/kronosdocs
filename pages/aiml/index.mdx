# Journey Into AI & ML

As an aspiring AI/ML enthusiast, my journey has been both challenging and exciting. I am constantly exploring, learning, and growing through a mix of online courses, personal projects, and community interactions. Here's a snapshot of my journey thus far and the path I've yet to tread.

## Completed:

- [x] **CS50's Introduction to Artificial Intelligence with Python by Harvard University**: This course laid the foundation of my understanding of AI. I learned about AI principles, key algorithms and techniques, and how to implement them in Python.
---
- [x] **Building Micrograd**: This project provided a step-by-step spelled-out explanation of backpropagation and training of neural networks, reinforcing my understanding of the fundamentals of neural networks.
---
- [x] **Building GPT**: My ultimate goal is to build a Generatively Pretrained Transformer (GPT), following OpenAI's GPT-2 / GPT-3.
## In Progress:

- [ ] **Building Makemore - Multilayer Perceptron (MLP)**: Currently, I am in the process of implementing an MLP character-level language model. This exercise is enhancing my understanding of machine learning basics like model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.  
---
- [ ] **Building Makemore - Bigram Character-level Language Model**: This exercise introduced me to `torch.Tensor` and its subtleties in efficiently evaluating neural networks. It also outlined the framework of language modeling that includes model training, sampling, and loss evaluation.
---
- [ ] **Understanding Activations & Gradients, BatchNorm in MLPs**: The next step in my journey is to delve into the internals of MLPs, scrutinize the statistics of forward and backward pass, and comprehend the pitfalls of improper scaling. 
## Yet to Complete:

- [ ] **Becoming a Backpropagation Ninja**: I aim to manually backpropagate through a 2-layer MLP without using PyTorch autograd's `loss.backward()`. This will improve my understanding of how gradients flow backwards through the compute graph.
---
- [ ] **Building a WaveNet**: I plan to deepen the 2-layer MLP into a tree-like structure, arriving at a convolutional neural network architecture similar to DeepMind's WaveNet.


Keep an eye on my journey as I continue to delve deeper into the exciting world of AI/ML!
